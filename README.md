
# aviv_etl ‚Äì Pipeline ETL PySpark vers PostgreSQL

Projet **config-driven** pour ing√©rer un jeu de donn√©es *listings* (CSV), appliquer des transformations (SQL ou Python),
faire des **contr√¥les qualit√©** (Great Expectations) puis **charger** les donn√©es en **Parquet** (silver) et en **PostgreSQL**
(gold) dans les tables  mod√©lis√© comme `property`, `historique_price` etc .

---

## üß≠ Vue d‚Äôensemble

- **Langage** : Python
- **Moteur** : PySpark (Spark local `local[4]`)
- **Qualit√© des donn√©es** : Great Expectations (validations param√©trables)
- **Stockage** :
  - **RAW / SILVER / GOLD** : chemins param√©tr√©s via `.env`
  - **Parquet** (silver) avec partition par `eventdate`
  - **PostgreSQL** (gold) via JDBC
- **Ex√©cution** : `python local.py` (choisit un job YAML dans `config/`)

---

## üóÇÔ∏è Structure du repo

```
aviv_etl/
‚îú‚îÄ local.py                       # point d‚Äôentr√©e : lance un pipeline √† partir d‚Äôun YAML
‚îú‚îÄ src/
‚îÇ  ‚îú‚îÄ execution_context.py        # lit .env + YAML, r√©sout les chemins, instancie Spark
‚îÇ  ‚îú‚îÄ wrapper_pipeline.py         # encha√Æne les jobs (GroupConfig) + timing & statut
‚îÇ  ‚îú‚îÄ pipeline.py                 # orchestration d‚Äôun job (inputs ‚Üí transforms ‚Üí quality_gate ‚Üí outputs)
‚îÇ  ‚îú‚îÄ extract/                    # Extractor g√©n√©rique (CSV/...) : sch√©ma DDL, options Spark
‚îÇ  ‚îú‚îÄ transform/                  # SQL ou fonctions Python (freestyle): normalisation, utils
‚îÇ  ‚îú‚îÄ load/                       # loaders Parquet & PostgreSQL (JDBC)
‚îÇ  ‚îú‚îÄ validators/                 # Great Expectations ‚Äì contr√¥les colonnes/r√®gles
‚îÇ  ‚îî‚îÄ utils/                      # Spark helper + lecture des fichiers YAML
‚îú‚îÄ config/
‚îÇ  ‚îú‚îÄ silver/listing.yaml         # job silver : RAW ‚Üí nettoyage ‚Üí Parquet (partition eventdate)
‚îÇ  ‚îî‚îÄ gold/listing.yaml           # job gold : silver ‚Üí quality gate ‚Üí Postgres
‚îú‚îÄ initdb/init_listing_table.sql  # DDL PostgreSQL (property, historique_price, dictionnaires)
‚îú‚îÄ docker-compose.yaml            # Postgres + pgAdmin
‚îú‚îÄ .env                           # variables (DB_*, RAW, SILVER, GOLD)
‚îú‚îÄ requirements.in
‚îî‚îÄ requirements.txt
```

---

## üîß Pr√©requis

- **Python** ‚â• 3.11 recommand√©
- **Java JDK** ‚â• 11 (requis par PySpark)
- **PostgreSQL** (local ou via Docker Compose)
- **pip** & **virtualenv** (recommand√©)

Paquets Python utiles (selon le code du projet) :
`pyspark`, `dataclasses-json`, `python-dotenv`, `great-expectations`, `pandas`, `pyyaml`

---

## ‚öôÔ∏è Configuration (.env)

Exemple de `.env` √† la racine du projet :

```env
# --- Base de donn√©es ---
DB_USER=db
DB_PASSWORD=db
DB_NAME=postgres_db
DB_HOST=localhost
DB_PORT=5432

# --- Data Lake local ---
RAW=./datalake/raw
SILVER=./datalake/silver
GOLD=./datalake/gold
```

> Cr√©ez les dossiers `./datalake/{raw,silver,gold}` et placez votre `listings.csv` dans `${RAW}`.

---

## üêò Base de donn√©es (Docker)

Un `docker-compose.yaml` est fourni pour Postgres + pgAdmin. D√©marrer :

```bash
docker compose up -d
# V√©rifier la sant√©
docker inspect --format='{{.State.Health.Status}}' postgres_db
```

Initialiser le sch√©ma (tables `property`, `historique_price`, etc.) :

```bash
# M√©thode 1 : psql local
psql "postgres://db:db@localhost:5432/postgres_db" -f initdb/init_listing_table.sql

# M√©thode 2 : via le conteneur
docker exec -i postgres_db psql -U db -d postgres_db < initdb/init_listing_table.sql
```

pgAdmin : http://localhost:5050 (admin@admin.com / admin). Ajoutez un serveur vers `postgres_db:5432` (user: `db`, pass: `db`).

---

## üì¶ Installation

```bash
python -m venv .venv
source .venv/bin/activate          # (Windows) .venv\Scripts\activate
pip install --upgrade pip

# D√©pendances projet
pip install -r requirements.txt    # (ou pip install -r requirements.in)

# D√©pendances souvent manquantes
pip install pyspark dataclasses-json python-dotenv great-expectations
```


---

## ‚ñ∂Ô∏è Ex√©cution

1) V√©rifiez `.env` (chemins RAW/SILVER/GOLD + DB).  
2) Placez **`listings.csv`** dans `${RAW}`.  
3) S√©lectionnez le job dans `local.py` :

```python
# local.py
context = ExecutionContext(
    # specification_file="config/silver/listing.yaml"
    specification_file="config/gold/listing.yaml"
)
```

- **Silver** : lit `${RAW}/listings.csv`, nettoie/normalise, √©crit en **Parquet** dans `${SILVER}/listing` (partition `eventdate=YYYY-MM-DD`).  
- **Gold** : lit la silver du jour (ex: `${SILVER}/listing/eventdate=2025-08-26`), ex√©cute **quality gate**, charge Postgres dans **`property`** et **`historique_price`**.

Lancez :

```bash
python local.py
```

> Le Spark local est g√©n√©ralement configur√© sur `local[4]`, m√©moire driver 4g.  
> Le connecteur JDBC Postgres peut √™tre charg√© via `spark.jars.packages=org.postgresql:postgresql:42.7.4`.

---

## üß™ Contr√¥les qualit√© (Great Expectations)

La section `quality_gate` des YAML (ex: `config/gold/listing.yaml`) d√©clenche un validateur (pr√©sence / nullit√© / types / cardinalit√©s).  
Les r√©sultats peuvent √™tre export√©s (events JSON).  
Pour d√©sactiver : commentez la section `quality_gate` du YAML gold.

---

## üß± Sch√©ma de donn√©es (Gold / PostgreSQL)

Le script `initdb/init_listing_table.sql` g√®re :

- Tables : `transaction_type`, `item_type`, `item_sub_type`
- **`property`** (colonnes : `id_property`, `start_date`, `price`, `area`, `site_area`, `floor`, `room_count`, `balcony_count`, `terrace_count`, `has_garden`, `city`, `zipcode`, `has_passenger_lift`, `is_new_construction`, `build_year`, `terrace_area`, `has_cellar`, `id_transaction_type`, `id_item_type`, `id_item_sub_type`)
- **`historique_price`** (suivi des changements de prix par `id_property`)


---

## üß© YAML de pipeline (structure)

Chaque job YAML contient :

- **`inputs`** : sources (format, options Spark, sch√©ma DDL, filtrage, s√©lection de colonnes)
- **`transforms`** : transformations en cha√Æne (SQL ou Python *freestyle*)
- **`quality_gate`** *(optionnel)* : validations Great Expectations
- **`outputs`** : loaders (Parquet et/ou Postgres)

Plusieurs jobs peuvent √™tre regroup√©s (GroupConfig) et ex√©cut√©s s√©quentiellement par un wrapper.

---

## üß∞ D√©bogage & probl√®mes fr√©quents

- **`ModuleNotFoundError: No module named 'pyspark'`** : installez `pyspark` et v√©rifiez **Java (JDK 11+)** + `JAVA_HOME`.
- **`great_expectations` introuvable** : `pip install great-expectations` ou commentez `quality_gate`.
- **Connexion Postgres refus√©e** : v√©rifiez Docker et `.env` (`DB_HOST`, `DB_PORT`, `DB_USER`, `DB_PASSWORD`, `DB_NAME`).
- **Chemins RAW/SILVER/GOLD** : cr√©ez les dossiers, placez `listings.csv` sous `${RAW}`.

---





## ‚úÖ R√©capitulatif rapide

```bash
# 1) Pr√©parer l‚Äôenv
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
pip install pyspark dataclasses-json python-dotenv great-expectations

# 2) D√©marrer Postgres
docker compose up -d
psql "postgres://db:db@localhost:5432/postgres_db" 

# 3) Configurer
# √âditez .env (RAW/SILVER/GOLD + DB_*)

# 4) Lancer
python local.py
```

